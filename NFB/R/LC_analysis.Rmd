---
title: "CENT LC analysis"
author: "Ben Cowley"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    theme: united
---

<!-- When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source('./znbnUtils.R')
source('./LC_analysis_functions.R')
source('./LC_visuals_functions.R')

##library(RColorBrewer)
##cols <- brewer.pal(8,"Dark2")
cols <- c("#1B9E77","#D95F02","#7570B3","#E7298A","#66A61E","#E6AB02","#A6761D","#666666")

library(dplyr)
library(tidyr)
library(data.table)
library(abind)
library(ggplot2)
library(corrgram)
```

# LC Analysis report

Aim of the approach is to quantify aspects or respresentations of learning (termed simply 'LCs' below) so that we can examine in more detail the relationship between patient's clinical activity and treatment outcome.

We use a fairly constrained definition of LC, as: _the demonstrated performance on a trial-to-trial basis in the specific task expicitly required of the patient_. 

### What LCs are not

We will distinguish task-learning from task-outcomes, even when such outcomes are measured repeatedly throughout the training, such as baseline bandpowers or repeated symptom self-reports. We will also not look at trial-wise changes in individual bandpowers, because the "_specific task expicitly required of the patient_" is to change __two__ bandpowers at the same time (also such data would require more EEG preprocessing, but we won't report that reason ;). 


### What LCs are

We will study the LCs representing the following aspects of learning:

* learning _gain_: how much learning happened = how much the performance score went up (growth curve model)
* _sufficient_ learning: how many sessions were required to learn = where is the plateau point inside a fixed number of sessions (quadratic fit)
* _monotonic_ learning: how probably do scores rise or fall monotonically, measured by:
    * session-wise rank-correlation of scores with order, _cor(S, O)_
    * aggregate over all sessions of: rank-correlation of trial scores with order, _A(S, cor(T, O))_
    * aggregate of both above
* _ideal_ learning: how consistent is observed trial-wise score correlation with 'ideal' learning. Measured by cosine similarity of trial-wise rank correlations to a hypothetical optimal learning curve derived from learning theory (Fitt's model)
<!-- * _rise_: session to session increase in performance scores (MWE of lagged subtraction) -->

[ _Names of LCs are arbitrary - some review of education literature might be required!_ ]


The results in draft paper "Learning Curves_v02.docx" show quantification for _gain_ and _sufficiency_, where session-wise scores were fitted with linear (growth curve) and quadratic models.

These two LC concepts are the main focus of prior work on NFB learning. When studies estimate whether subjects have learned, they usually calculate _gain_ in some way (REF). Several studies have also estimated _sufficiency_ by looking at the number of sessions required to see a plateau in improvement (REF).

However, such fitted models are very sensitive to small changes in the data: outliers can change the linear-fit slope or the curvature of the quadratic fit by significant amounts. Thus it is valuable to also look at LCs that are model-free, i.e. non-parametric. 

Our _monotonic_ and _ideal_ LCs address this issue, and provide clear and easy-to-interpret models of learning. 


### Summary

Our analysis approach is thus to use the derived LCs to discover group-wise patterns in the various background and outcome variables available, in a within-subjects manner.

In the rest of the report, we will step through the methods for creating _monotonic_, and _ideal_ LCs to see how they work. We will then explore the relationships with background and outcome variables. First, we describe the data used.



## Data
We work primarily from the following datasets (available in shared Dropbox folder or on request):

 * auto_block.csv - _trial-wise scores, game-types, and date-time values_
 * test_outcomes.csv - _per-subject values for background, test outcomes, and self-reports_
 * meta_data.csv - _aggregate values for all sessions in NFB training_

We filter trials to clean the dataset, removing the first session (as it was a training session), trials with score = 0, and trials marked bad by trainers.

```{r load_data, echo=FALSE, warning=FALSE}

pth <- '~/Dropbox/PROJECT_CENT/CENT_patients/CENT_DB_2013-08-27/Sessions/'

# Load the outcome and meta-data
nfb <- read.csv('test_outcomes.csv', sep="\t")
nfb <- nfb[order(nfb[,1]),]
mta <- read.csv(paste0(pth, 'meta_data.csv'), sep=",")
mta <- mta[order(mta[,1]),]

# Derive some indexing values from the datasets
n.NOR <- 38
n.INV <- 16
n.TRA <- 8
SBJS <- unique(nfb$Part_number)
n.SBJS <- NCOL(SBJS) * NROW(SBJS)
TB <- unique(subset(SBJS, nfb$TB.1.SMR1 == -1))
SMR <- unique(subset(SBJS, nfb$TB.1.SMR1 == 1))
MEN <- unique(subset(SBJS, nfb$Female0.Male2 == 2))
WMN <- unique(subset(SBJS, nfb$Female0.Male2 == 0))
DV <- "adj_score"

# Get the most important data: trials/blocks, and make needed variables
tr.raw <- read.csv(paste0(pth, 'auto_block.csv'), sep=";")
tr.blk <- droplevels(subset(tr.raw, session_num > 1 & score != 0 & trainer_says_no != 1 & patient %in% SBJS))
tr.blk <- tr.blk[order(tr.blk[,1]),]
tr.blk$secs <- as.numeric(strptime(tr.blk$date_time, format='%Y:%m:%d %H:%M:%S'))

# Print summary of ALL trials in ALL categories
printTrials(tr.raw, DV, TB, SMR)
printTrials(tr.blk, DV, TB, SMR)
```


We also subset scores according to the three training modes (normal, inverse, transfer), so we have four (clean) datasets (further subdivided by subjects in TB protocol and subjects in SMR protocol):

* all trials
* normal trials
* inverse trials
* transfer trials

For each dataset we create a new index of sessions so that rows can be aligned according to how many sessions of a protocol were conducted, as opposed to what the session number was when starting. Thus, e.g. transfer trials are indexed from 1 to 10 for all subjects, regardless of what session it was that their transfer trials really started (first transfer trial ranged from session 28 to session 35, depending on subject).


```{r parse trials data, echo=FALSE, warning=FALSE}

# Subset the block data for analysis. Patients may have different start sessions for inverse, transfer.
trials <- getAllTrials(tr.blk)
tr.nor0 <- getNorTrials(tr.blk) # Normal trials only
tr.inv0 <- getInvTrials(tr.blk) # Inverse trials only
tr.tra0 <- getTraTrials(tr.blk) # transfer trials only

# Print summary of ALL trials in ALL categories
printTrials(trials, DV, TB, SMR)
printTrials(tr.nor0, DV, TB, SMR)
printTrials(tr.inv0, DV, TB, SMR)
printTrials(tr.tra0, DV, TB, SMR)
```


We come across our first significant problem. For calculating trial-wise correlations, we require 2 or more trials per session. Many sessions contain only one trial of a certain type, especially for transfer trials. We therefore create datasets that prune out the sessions with trials = 1.
We also face a constraint when we calculate the cosine similarity of trial-wise correlations with a hypothetical learning curve: the number of sessions should be enough to accommodate the definition of the hypothetical curve (e.g. 3+ for Fitt's model).


```{r parse trials data no singletons, echo=FALSE, warning=FALSE}

tr.nor <- getNorTrials(tr.blk, min_trial = 1) # Normal trials only, cut sessions with 1 trial
tr.inv <- getInvTrials(tr.blk, min_trial = 1) # Inverse trials only, cut sessions with 1 trial
tr.tra <- getTraTrials(tr.blk, min_trial = 1) # transfer trials only, cut sessions with 1 trial

# Print summary of ALL trials in ALL categories with single-trial sessions removed
printTrials(tr.nor, DV, TB, SMR)
printTrials(tr.inv, DV, TB, SMR)
printTrials(tr.tra, DV, TB, SMR)
```


However, pruning out sessions with 1 trial results in losing quite a lot of data. The total number of sessions per subject for each training mode now varies quite a bit: this is no problem because our LC calculation methods are not sensitive to small differences in N, except when N is very small (see more below). However, for the transfer training mode, removing sessions with 1 trial results in losing entire subjects. For this reason, because session LCs and trial-wise LCs are not measuring the same thing, we can use different datasets for each: for session LCs, we will include sessions with 1 trial.

Session data for calculating session LCs is defined as the median of trial-wise scores per session.

```{r parse session data, echo=FALSE, warning=FALSE}
summary_method = "median"

sn.nor <- getSSN(tr.nor0, DV, summary_method) # Normal sessions only
sn.inv <- getSSN(tr.inv0, DV, summary_method) # Inverse trials only
sn.tra <- getSSN(tr.tra0, DV, summary_method) # transfer trials only


# Print summary of ALL trials in ALL categories with single-trial sessions removed
printTrials(sn.nor, DV, TB, SMR)
printTrials(sn.inv, DV, TB, SMR)
printTrials(sn.tra, DV, TB, SMR)
```


## Measuring _monotonic_ LC

Kendall rank correlations provide a quantification of _monotonicity_ (we will use Kendall rather than Spearman for rank correlation as it is recommended for low N - the interpretation remains the same). The outcome range is -1 to 1, where monotonic increase in score per session results in Kendall t=1, and monotonic decrease results in Kendall t=-1. Presumably, monotonic increase in performance scores is a positive sign for learning. Let's call this the Monotonic LC (MonoLC).

### Session-wise MonoLC
The first part of MonoLC is the Kendall rank-correlation of session number with adjusted score per 1-hour 'daily' session. Variable names (as used in visuals and testing) are listed below with summary stats; not all variables are later used however.

```{r session Mono LC, echo=FALSE, warning=FALSE}
# BY SESSIONS: Kendall correlations of daily session adjusted score with session number
ssn.trl.MLC <- getSsnMLC(trials, DV, summary_method)
ssn.nor.MLC <- getSsnMLC(tr.nor0, DV, summary_method)
ssn.inv.MLC <- getSsnMLC(tr.inv0, DV, summary_method)
ssn.tra.MLC <- getSsnMLC(tr.tra0, DV, summary_method)
```

"ssn.trl.MLC" = monotonic LC for sessions including ALL trial data:
```{r session all trials MLC summary, echo=FALSE, warning=FALSE} 
summary(ssn.trl.MLC$ssn.MLC)
```

"ssn.nor.MLC = monotonic LC for sessions including only NORMAL trial data:
```{r session normal trials MLC, echo=FALSE, warning=FALSE}
summary(ssn.nor.MLC$ssn.MLC)
```

"ssn.inv.MLC" = monotonic LC for sessions including only INVERSE trial data:
```{r session inverse trials MLC, echo=FALSE, warning=FALSE}
summary(ssn.inv.MLC$ssn.MLC)
```

"ssn.tra.MLC" = monotonic LC for sessions including only TRANSFER trial data
```{r session transfer trials MLC, echo=FALSE, warning=FALSE}
summary(ssn.tra.MLC$ssn.MLC)
```

### Trial-wise MonoLC
To measure performance across trials, we take Kendall rank-correlation of trial order with trial-wise score, inside each session. This gives a vector for each subject, of length = number of sessions. We can reduce the vector to a scalar value using an unbiased aggregation estimator provided by Hunter-Schmidt (see Zhang & Wang 2014, Multivariate Behavioral Research, 49:2, 130-148; Hunter-Schmidt is preferred for rank correlations because it will not crash when t=+-1, nor when N<4).


```{r trial Mono LC, echo=FALSE, warning=FALSE}
corvar <- 'secs'

# MONOTONIC of LEARNING BY TRIALS: 
# Kendall correlations of per-trial adjusted score with order
trials.MLC <- getTrialAggCor(trials, DV, corvar)
tr.nor.MLC <- getTrialAggCor(tr.nor, DV, corvar)
tr.inv.MLC <- getTrialAggCor(tr.inv, DV, corvar)
tr.tra.MLC <- getTrialAggCor(tr.tra, DV, corvar)
```

"trials.MLC" = monotonic LC for ALL trials data:
```{r all trials MLC summary, echo=FALSE, warning=FALSE} 
summary(trials.MLC$ret.agg)
```

"tr.nor.MLC" = monotonic LC for NORMAL trials data:
```{r normal trials MLC, echo=FALSE, warning=FALSE}
summary(tr.nor.MLC$ret.agg)
```

"tr.inv.MLC" = monotonic LC for INVERSE trials data:
```{r inverse trials MLC, echo=FALSE, warning=FALSE}
summary(tr.inv.MLC$ret.agg)
```

"tr.tra.MLC" = monotonic LC for TRANSFER trials data
```{r transfer trials MLC, echo=FALSE, warning=FALSE}
summary(tr.tra.MLC$ret.agg)
```


### Aggregate MonoLC
We aggregate the session- and trial-wise correlations using Hunter-Schmidt.

```{r Monotonic LC, echo=FALSE, warning=FALSE}
## MONOTONIC LEARNING
MonoLC <- aggMLC(trials.MLC, ssn.trl.MLC)
MLC.nor <- aggMLC(tr.nor.MLC, ssn.nor.MLC)
MLC.inv <- aggMLC(tr.inv.MLC, ssn.inv.MLC)
MLC.tra <- aggMLC(tr.tra.MLC, ssn.tra.MLC)

```

"MonoLC" = aggregate monotonic LC for ALL trials data:
```{r aggr all trials MLC summary, echo=FALSE, warning=FALSE} 
summary(MonoLC$aggMLC)
```

"MLC.nor" = aggregate monotonic LC for NORMAL trials data:
```{r aggr normal trials MLC summary, echo=FALSE, warning=FALSE}
summary(MLC.nor$aggMLC)
```

"MLC.inv" = aggregate monotonic LC for INVERSE trials data:
```{r aggr inverse trials MLC summary, echo=FALSE, warning=FALSE}
summary(MLC.inv$aggMLC)
```

"MLC.tra" = aggregate monotonic LC for TRANSFER trials data
```{r aggr transfer trials MLC summary, echo=FALSE, warning=FALSE}
summary(MLC.tra$aggMLC)
```


## Measuring trial-wise _Skill Acquisition_  LC
The preferred hypothetical learning curve follows Fitt's model (REF - see Edua's theory text 07.06.2017). Our choice of Fitt's model is printed below. The choice of model impacts the testing outcomes very strongly.

Fitt's model has three phases. Thus the smallest possible number of sessions is three, and for this reason some more subjects will be pruned in the sparse transfer trial data, because they have only two or one session with >1 transfer trial. This causes problems for aggregating or comparing the datasets, as you cannot create, e.g., a correlation matrix of unequal-sized vectors.
Cosine similarity ranges from 1 to -1.

### Fitts model for monotone improvement

Working again with the Trial-wise MonoLC vector, we reduce it to a scalar value by taking the cosine similarity of each vector and an equal-length vector representing a hypothetical ideal learning curve. For example, a vector of 1s represents the hypothetical performance where trial-wise scores increased monotonically consistently across all sessions.

```{r Fitts monotone LC, echo=FALSE, warning=FALSE}
FITTs <- c(0, 1, 0.5)
print(paste("Fitt's model for monotone improvement = ", toString(FITTs)))

# Kendall correlations of per-trial adjusted score with order - Kendall is used for small N
trial.cors <- getTrialCors(trials, DV, corvar)
tr.nor.cor <- getTrialCors(tr.nor, DV, corvar)
tr.inv.cor <- getTrialCors(tr.inv, DV, corvar)
tr.tra.cor <- getTrialCors(tr.tra, DV, corvar)

# IDEAL: cosine similarity of trial cors to an arbitrary hypothetical LC (AHLC)
IdealLC <- getTrialCosSim(trial.cors[,-(1)], trial.cors[,1], AHLC = "Fitts", phase123 = FITTs)
ILC.nor <- getTrialCosSim(tr.nor.cor[,-(1)], tr.nor.cor[,1], AHLC = "Fitts", phase123 = FITTs)
ILC.inv <- getTrialCosSim(tr.inv.cor[,-(1)], tr.inv.cor[,1], AHLC = "Fitts", phase123 = FITTs)
ILC.tra <- getTrialCosSim(tr.tra.cor[,-(1)], tr.tra.cor[,1], AHLC = "Fitts", phase123 = FITTs)
```

"IdealLC" = ideal LC for ALL trials data:
```{r all trials ideal LC summary, echo=FALSE, warning=FALSE} 
summary(IdealLC[,2])
```

"ILC.nor" = ideal LC for NORMAL trials data:
```{r normal trials idealLC summary, echo=FALSE, warning=FALSE}
summary(ILC.nor[,2])
```

"ILC.inv" = ideal LC for INVERSE trials data:
```{r inverse trials idealLC summary, echo=FALSE, warning=FALSE}
summary(ILC.inv[,2])
```

"ILC.tra" = ideal LC for TRANSFER trials data
```{r transfer trials idealLC summary, echo=FALSE, warning=FALSE}
summary(ILC.tra[,2])
```


### Fitts model for geometric mean score

```{r Fitts geom mean LC, echo=FALSE, warning=FALSE}
FITTs <- c(-0.5, 0, 0.5)
print(paste("Fitt's model for geometric mean score = ", toString(FITTs)))

# centered to 0, scaled to [-1..1], geometric mean scores per session
trial.scor <- getTrialCSGmean(trials, DV)
tr.nor.scr <- getTrialCSGmean(tr.nor, DV)
tr.inv.scr <- getTrialCSGmean(tr.inv, DV)
tr.tra.scr <- getTrialCSGmean(tr.tra, DV)

# IDEAL: cosine similarity of trial cors to an arbitrary hypothetical LC (AHLC)
IdealLC <- getTrialCosSim(trial.scor[,-(1)], trial.scor[,1], AHLC = "Fitts", phase123 = FITTs)
ILC.nor <- getTrialCosSim(tr.nor.scr[,-(1)], tr.nor.scr[,1], AHLC = "Fitts", phase123 = FITTs)
ILC.inv <- getTrialCosSim(tr.inv.scr[,-(1)], tr.inv.scr[,1], AHLC = "Fitts", phase123 = FITTs)
ILC.tra <- getTrialCosSim(tr.tra.scr[,-(1)], tr.tra.scr[,1], AHLC = "Fitts", phase123 = FITTs)
```


## Examine  _monotonic_ and _ideal_ LCs

### Plotting

It is useful to look at the LC data in raw form. The _monotonic_ and _ideal_ LCs have the same range [1, -1] and similar interpretation (-1..1 = distance from perfect performance according to the model being used). Thus, we examine the two LCs sideby-side in the same plot. As we see, _monotonic_ LCs tend to have a lower variance than _ideal_ LCs, probably because the _monotonic_ calculation involves more aggregation. For the normal and inverse modes, _monotonic_ has a higher mean than _ideal_, but it is not likely to be significant by visual inspection.

```{r trial LC, echo=FALSE, warning=FALSE}
df <- as.data.frame(cbind(MonoLC[,2], IdealLC[,2], 
                          MLC.nor[,2], ILC.nor[,2], 
                          MLC.inv[,2], ILC.inv[,2],
                          c(MLC.tra[,2], rep(NA, 6)), c(ILC.tra[,2], rep(NA, 10))))
names(df) <- c("MonoLC", "IdealLC", "MLC-nor", "ILC-nor", "MLC-inv", "ILC-inv", "MLC-tra", "ILC-tra")
ggplot(melt(df), aes(factor(variable), value)) + geom_boxplot() + 
  geom_point(aes(color = factor(variable)), position = position_dodge(width = 0.5))


```



We can also sort the main LCs and view scatterplots to get a sense of how clustered the results are. 

```{r learing LC, echo=FALSE, warning=FALSE}
par(mfrow = c(1,3))

plot(sort(ssn.trl.MLC[,2]), ylab = "sorted score", main = "session-wise MonoLC")
abline(h = 0.5, lty = 3)

plot(sort(trials.MLC[,2]), ylab = "", main = "trial-wise MonoLC")
abline(h = 0.5, lty = 3)

plot(sort(IdealLC[,2]), ylab = "", main = "Cos-sim Ideal LC")
abline(h = 0.5, lty = 3)

par(mfrow = c(1,1))
```


We can explore how the _monotonic_ and _ideal_ LCs relate to each other and to the following variables:

 * Demographics, TB vs SMR, co-morbidities, BIS/BAS
 * ADHD symptom self-reports
 * IQ components
 * Vigilance pre-test EEG measurement
 
We do this using a correlation matrix plot. Variables with their ranges lie on the diagonal. Over the diagonal are correlations of variable-pairs, and confidence intervals (in parentheses). Under the diagonal are loess-curve fits to the scatter plots of variable-pairs.

```{r ideal, monotonic LC explorations, echo=FALSE, warning=FALSE}
corrMatrix("MonoLC", MonoLC[,2],
           "session.MLC", ssn.trl.MLC[,2], 
           "trial.MLC", trials.MLC[,2],
           "IdealLC", IdealLC[,2],
           title = "Ideal, Monotonic learning")

corrMatrix("MonoLC", MonoLC[,2],
           "IdealLC", IdealLC[,2],
           "MLC.nor", MLC.nor[,2],
           "MLC.inv", MLC.inv[,2],
           "ILC.nor", ILC.nor[,2],
           "ILC.inv", ILC.inv[,2],
           "TB-1/SMR1", nfb$TB.1.SMR1,
           title = "Ideal, Monotonic x Training Modes, Protocol")

corrMatrix("MonoLC", MonoLC[,2],
           "IdealLC", IdealLC[,2],
           "age", nfb$Age,
           "sex", nfb$Gender..1.F..2.M.,
           "BAS", apply(rbind(nfb$BAS.Drive, nfb$BAS.Fun.Seeking, nfb$BAS.Reward.Responsiveness), 2, mean), 
           "BIS", nfb$BIS, 
           "comorbid", nfb$scales.COMORBID,
           title = "Ideal, Monotonic LCs vs background")

corrMatrix("MonoLC", MonoLC[,2],
           "IdealLC", IdealLC[,2],
           "diagnosis", nfb$diagnosis..1.ADHD..2.ADD.,
           "diag. comorbid", nfb$diagnostic.COMORBID,
           "ASRS", nfb$ASRS_total,
           "ASRS-I", nfb$ASRS_inattention_score, 
           "ASRS-H", nfb$ASRS_hyperactivity.impulsivity_score, 
           "BADDS", nfb$BADDS,
           title = "Ideal, Monotonic LCs vs diagnosis")

corrMatrix("MonoLC", MonoLC[,2],
           "IdealLC", IdealLC[,2],
           "IQ", nfb$FSIQ,
           "verbal IQ", nfb$VIQ,
           "performance IQ", nfb$PIQ,
           "digit span", nfb$Span_total_standardized, 
           "forward WM span", nfb$Span_fwd, 
           "backward WM span", nfb$Span_bwd,
           title = "Ideal, Monotonic LCs vs IQ")

```



```{r load_vigilance_data, echo=FALSE, warning=FALSE}

datpath <- "~/Dropbox/PROJECT_CENT/Analysis/Vigilance"
vigi_in = read.table(file.path(datpath, "VIGALL_Intake.csv"), header=TRUE, sep="\t", row.names=1)
vigiPT<-subset(vigi_in, Group=="PT" & as.numeric(row.names(vigi_in)) %in% SBJS)
vigiPWL<-subset(vigi_in, Group=="PWL")
vigiC<-subset(vigi_in, Group=="C")


idx <- SBJS %in% as.numeric(row.names(vigiPT))
corrMatrix("MonoLC", MonoLC[idx,2],
           "IdealLC", IdealLC[idx,2],
           "lability", vigiPT$Index_W,
           "waking-stage %", vigiPT$prW,
           "A-stage %", vigiPT$prA,
           "B-stage %", vigiPT$prB, 
           "C-stage %", vigiPT$prC, 
           "artefacts", vigiPT$Art,
           title = "Ideal, Monotonic LCs vs Vigilance")

```


Let's look a bit closer at the major finding from all these matrices. Protocol and LC correlate strongly. Some boxplots can show this a bit clearer.

```{r Ideal, Monotonic LCs vs protocol, sex, echo=FALSE, warning=FALSE}
par(mfrow=c(1,3))
LC <- rbind(MonoLC[,2], IdealLC[,2])
ttl <- c("Monotonic", "Ideal")
for (idx in 1:2)
{
  boxplot(LC[idx,] ~ nfb$Gender..1.F..2.M., main = paste(ttl[idx], "v sex"), names = c("F", "M"))
  boxplot(LC[idx,] ~ nfb$TB.1.SMR1, main = paste(ttl[idx], "v protocol"), names = c("TB", "SMR"))
  boxplot(LC[idx,] ~ nfb$Gender..1.F..2.M. * nfb$TB.1.SMR1, main = paste(ttl[idx], "v \nprotocol x sex"), names = c("F.TB", "M.TB", "F.SMR", "M.SMR"))
}

```

### Testing Monotonic LC

First, we want to know whether the group-wise Monotonic LC distribution was significantly biased, i.e. does the LC represent a consistent pattern of change across the group (do they improve)? The LC is based on calculations of Kendall's tau. Under the null hypothesis of independence of X and Y, the sampling distribution of tau has an expected value of zero. Thus, group-wise null hypothesis can be that the distribution of LCs has a mean value close to zero. Because we have a small sample, we will not assume that the LC has a normal distribution, but instead use Wilcoxon's non-parametric test.

```{r test Monotonic LC significance, echo=FALSE, warning=FALSE}
wilcox.test(MonoLC[,2], exact = TRUE, conf.int = TRUE)
wilcox.test(MLC.nor[,2], exact = TRUE, conf.int = TRUE)
wilcox.test(MLC.inv[,2], exact = TRUE, conf.int = TRUE)
wilcox.test(MLC.tra[,2], exact = TRUE, conf.int = TRUE)

```

Here we fit a linear regression model to Monotonic LCs to test the effect of protocol (TB, SMR). We also test the interaction of protocol by sex. A linear model is fitted to the LC for all modes together; and to each training mode (normal, inverse, transfer) separately. 

Finally the 8th and 10th tests are performed for monotonic LCs from session data and trial data separately.

__NOTE__: In total, for MonoLCs and IdealLCs, there are ten linear models reported, thus a very simple mental correction for multiplicity (equivalent to Bonferroni) can be performed by moving the decimal point one place right.

```{r Linear models of Monotonic LCs vs protocol * sex, echo=FALSE, warning=FALSE}
# MONOTONIC LEARNING
lm.MLC <- lm(MonoLC[,2] ~ nfb$TB.1.SMR1)
summary(lm.MLC)
lm.MLC.nor <- lm(MLC.nor[,2] ~ nfb$TB.1.SMR1)
summary(lm.MLC.nor)
lm.MLC.inv <- lm(MLC.inv[,2] ~ nfb$TB.1.SMR1)
summary(lm.MLC.inv)
idx <- nfb$Part_number %in% MLC.tra[,1]
lm.MLC.tra <- lm(MLC.tra[,2] ~ nfb[idx,]$TB.1.SMR1)
summary(lm.MLC.tra)

lm.MLC <- lm(MonoLC[,2] ~ nfb$TB.1.SMR1 * nfb$Female.2.Male2)
summary(lm.MLC)
lm.MLC.nor <- lm(MLC.nor[,2] ~ nfb$TB.1.SMR1 * nfb$Female.2.Male2)
summary(lm.MLC.nor)
lm.MLC.inv <- lm(MLC.inv[,2] ~ nfb$TB.1.SMR1 * nfb$Female.2.Male2)
summary(lm.MLC.inv)
idx <- nfb$Part_number %in% MLC.tra[,1]
lm.MLC.tra <- lm(MLC.tra[,2] ~ nfb[idx,]$TB.1.SMR1 * nfb[idx,]$Female.2.Male2)
summary(lm.MLC.tra)

lm.ssn.MLC <- lm(ssn.trl.MLC[,2] ~ nfb$TB.1.SMR1 * nfb$Female.2.Male2)
summary(lm.ssn.MLC)
lm.trials.MLC <- lm(trials.MLC[,2] ~ nfb$TB.1.SMR1 * nfb$Female.2.Male2)
summary(lm.trials.MLC)
```


### Testing Ideal LC

We also test whether the group-wise Ideal LC distribution was significantly biased, i.e. do they learn a skill? As before, we use Wilcoxon's non-parametric test.

```{r test Ideal LC significance, echo=FALSE, warning=FALSE}
wilcox.test(IdealLC[,2], exact = TRUE, conf.int = TRUE)
wilcox.test(ILC.nor[,2], exact = TRUE, conf.int = TRUE)
wilcox.test(ILC.inv[,2], exact = TRUE, conf.int = TRUE)
wilcox.test(ILC.tra[,2], exact = TRUE, conf.int = TRUE)

```

Next we fit linear regression models to the Ideal LCs. First four are effect of protocol only, next four are interaction effect of protocol x sex. Data from all modes is tested together, then each training mode is tested separately.


```{r Linear models of Ideal LCs vs protocol * sex, echo=FALSE, warning=FALSE}
# IDEAL LEARNING
lm.ILC <- lm(IdealLC[,2] ~ nfb$TB.1.SMR1)
summary(lm.ILC)
lm.ILC.nor <- lm(ILC.nor[,2] ~ nfb$TB.1.SMR1)
summary(lm.ILC.nor)
lm.ILC.inv <- lm(ILC.inv[,2] ~ nfb$TB.1.SMR1)
summary(lm.ILC.inv)
idx <- nfb$Part_number %in% ILC.tra[,1]
lm.ILC.tra <- lm(ILC.tra[,2] ~ nfb[idx,]$TB.1.SMR1)
summary(lm.ILC.tra)

lm.ILC <- lm(IdealLC[,2] ~ nfb$TB.1.SMR1 * nfb$Female.2.Male2)
summary(lm.ILC)
lm.ILC.nor <- lm(ILC.nor[,2] ~ nfb$TB.1.SMR1 * nfb$Female.2.Male2)
summary(lm.ILC.nor)
lm.ILC.inv <- lm(ILC.inv[,2] ~ nfb$TB.1.SMR1 * nfb$Female.2.Male2)
summary(lm.ILC.inv)
idx <- nfb$Part_number %in% ILC.tra[,1]
lm.ILC.tra <- lm(ILC.tra[,2] ~ nfb[idx,]$TB.1.SMR1 * nfb[idx,]$Female.2.Male2)
summary(lm.ILC.tra)
```


## Other data
___Work in Progress - to be continued...___

We would also like to look at individual band powers from per-session baselines.

We can also consider comparing some other data that takes a bit more work to include:

 * IAPF (but prior work relating this to learning outcome was for alpha-power training, which CENT was not)
 * Pre-test theta/beta bandpowers (but protocols were assigned based on this, so it is confounded)
 * Pittsburgh Sleep Quality Index (PSQI) (asked 4 times throughout training)
 * Placebo survey (asked 3 times throughout training)
 * T.O.V.A. variables, pre and/or post (already thoroughly investigated?)
 * Trainer-patient interaction self-reports (not validated instruments)



## Other LC methods - temporal difference
Another way to examine signal change without fitting a model is to look at autocorrelation or lagged difference. 

___Work in Progress - to be continued...___

