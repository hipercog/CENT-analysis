---
title: "CENT LC analysis"
author: "Ben Cowley"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source('./znbnUtils.R')
source('./LC_analysis_functions.R')
source('./LC_visuals_functions.R')

##library(RColorBrewer)
##cols <- brewer.pal(8,"Dark2")
cols <- c("#1B9E77","#D95F02","#7570B3","#E7298A","#66A61E","#E6AB02","#A6761D","#666666")

library(plyr)
library(dplyr)
library(tidyr)
library(data.table)
library(abind)
library(ggplot2)
library(corrgram)
library(psych)
library(nlme)
library(broom)
```

# LC Analysis report - Introduction & Research Questions

Our overall aim is to study what KIND of learning occurs in NF, so that we can examine in more detail the relationship between patient's clinical activity and treatment outcome. This suggests two foci: (A) developing modelling methods to perform a more thorough examination of learning than any previously; (B) applying the developed models of learning to our clinical trial data to tease out the mechanisms of different protocols and modes of training.

Prior work seems to have focused on overall performance improvement, mainly so that subjects can be split into groups of learners and non-learners, to estimate the efficacy of treatment for learners only. This is a neat trick when studying NF efficacy, but not very useful to study learning itself.

In order to develop methods to examine learning, we must study performance at trial, session, and treatment level; and measure performance both in terms of magnitude and the pattern of change. To achieve this, we derive data that robustly captures (1) magnitude and (2) change for all normal and transfer trials (details below). Normal and transfer are joined in order to minimise difference in __number__ of trials per session across treatment (which changes because inverse trials are introduced halfway). 

Thus, in part A, our data is within-subjects 'learning curves' (LCs), defined as: _performance (1) magnitude and (2) change computed from trials within each session, for all sessions_. 

In part B the data can be subdivided into separate protocols and training modes, to explore the details of how they differ in terms of the best-fit learning model(s) (para and non-para). The split between A and B helps distinguish task-learning from task-outcomes, even when such outcomes are measured repeatedly throughout the training, such as baseline bandpowers or repeated symptom self-reports.



## A. Developing modelling methods

Following the literature (Fitts, Posner), we investigate whether NF shows evidence of being skill acquisition, so going beyond operant conditioning. This would be important also clinically, because conditioning could be conceivably automated and made into, e.g. an 'app'; but skill acquisition requires coaching.

To investigate learning, we must deal with the complexity of _analysing_ learning: there are different qualities of learning captured by different sorts of analysis, e.g.

* _magnitude_ of change in overall performance (increase from beginning to end)
* _consistency_ of performance improvement (monotonicity within sessions and trials)
* _shape_ of performance curve:
     * linear
     * power law (or piecewise power law)
     * exponential or sigmoidal
* _plateau point_ of performance improvement across sessions

The results in draft paper __"Learning Curves_v02.docx"__ show quantification for _magnitude_ and _plateau point_, where session-wise scores were fitted with linear (growth curve) and quadratic models. These two LC concepts are the main focus of prior work on NFB learning. When studies estimate whether subjects have learned, they usually calculate _gain_ in some way (REF). Several studies have also estimated _sufficiency_ by looking at the number of sessions required to see a plateau in improvement (REF).
     
Then there is the issue of how to quantify aspects or respresentations of learning. Parametric methods include linear regression, curve fitting, and hierarchical models. These can be useful but also sensitive to violations of assumptions, which can be hard to avoid in noisy data. Non-parametric approaches can help, and we develop one based on cosine similarity between performance metrics and canonical learning curves.

### A: Research Questions

First, it's important to know about overall performance improvement for the purpose of context. If the scores of all trials in all types of training tend to increase, that tells us that learning _of some kind_ must have happened, and we can proceed to study _what kind_.

 * RQ1 - _overall performance __magnitude__: do scores grow from beginning to end of treatment?_

It's also interesting to check if learning has plateaued or not, by fitting a quadratic curve and checking the sign of the quadratic term. 

 * RQ2 - _plateau of performance improvement reached? Do performance scores reach a high point before the end of the training, and hold steady? Or are they still growing by the end of treatment?_
 
    
As noted above, a linear of model of learning does not fit to learning theory nor provide much more information than that performance increased or decreased. Other curve families have been used to describe learning with better empirical support. Power law curves were long thought to best describe learning due to practice (Newell, Rosenbloom, 1980). However, other curve families have been argued to fit better to non-averaged (individual) curves: e.g. exponential (Heathcote et al, 1999). 
Further, if performance conforms to a multi-stage profile, e.g. as the three stage model for motor skill learning (Fitts, Posner), then a piecewise power law model can fit better. And the type of task-reward that the data come from can also affect, e.g. in success-only tasks, a sigmoid curve can fit the data (Leibowitz et al 2010), and notably a sigmoid arguably consists of three phases, relating to Fitts-Posner.

 * RQ3 - _what kind of learning? Which family of curves fits best to the data (by parametric modelling)?_

These parametric approaches have some issues: violations of their assumptions, such as outliers, can be hard to avoid in noisy data. Some fitted models are very sensitive to small changes in the data: outliers can change the linear-fit slope or the shape of a fitted curve by significant amounts. Also (perhaps most importantly), treatment-level curve-fitting is blind to intra-session patterns.
Thus it is valuable to also look at LCs that are model-free, i.e. non-parametric, and can account flexibly for intra-session variability. We develop non-parametric LCs to provide clear and easy-to-interpret models of learning that are easy to adjust to diverse theories (i.e. cosine similarity).

 * RQ4 - _what kind of learning? Is the skill acquisition theory supported by our novel non-parametric model?_




## B. mechanisms of TB v SMR protocols, normal/inverse/transfer modes of training, and other covariates

When the best fitting learning model(s) are established, we can use them to explore the real complexity of the CENT NF data, including TB and SMR protocols, and normal, inverse, transfer training modes. Each of the protocols have theoretical grounding in cortical arousal and motoric activation regulation. Thus, we can also relate these sub-groups to the baseline bandpowers per session, and the pre-test baseline vigilance analysis and per-session sleep self-reports. 


### B: Research Questions

Learning in protocols

 * RQ5 - compare TB and SMR protocols on best-fit learning model(s)
    * Method - graphical and statistical comparison of TB vs SMR
    * Test - test distribution difference for scalar features or complete learning curve time series
        * Kolmogorov-Smirnov test for scalar features, e.g. cosine similarity
        * Maximum-Width Envelope test for complete learning curves / time-series
    * __TODO__ - pending part A, apply best-fit learning model and derive features to test

Learning in training modes

 * RQ6 - compare normal, inverse, transfer (and combinations?) training modes on best-fit learning model(s)
    * Method - graphical and statistical comparison of training modes
    * Test - test distribution difference for scalar features or complete learning curve time series
        * Kolmogorov-Smirnov test for scalar features, e.g. cosine similarity
        * Maximum-Width Envelope test for complete learning curves / time-series
    * __TODO__ - pending part A, apply best-fit learning model and derive features to test

Learning related to session-wise baseline bandpowers

 * RQ7 - learning model(s) moderate or are moderated by the baseline bandpowers
    * Method - 
    * Test - 
    * __TODO__ - 


Learning related to vigilance baseline and sleep self-reports

 * RQ8a - learning model(s) are moderated by baseline vigilance classification from EEG
    * Method - 
    * Test - 
    * __TODO__ - 
 * RQ8b - learning model(s) are moderated by, or moderate, sleep self-reports
    * Method - 
    * Test - 
    * __TODO__ - 



## Summary

Our analysis approach has two parts: (A) investigate models of learning in NF data to find best-fit LCs; and (B) use the derived LCs to discover group-wise patterns in the various background and outcome variables available, in a within-subjects manner.

In the rest of the report, we will step through the methods for creating all LCs, to see how they work. We will then explore the relationships with background and outcome variables. First, we describe the data used.



# LC Analysis report - Data Preparation
We work primarily from the following datasets (available in shared Dropbox folder or on request):

 * auto_block.csv - _trial-wise scores, game-types, and date-time values_
 * test_outcomes.csv - _per-subject values for background, test outcomes, and self-reports_
 * meta_data.csv - _aggregate values for all sessions in NFB training_

The raw data is saved as 'tr.raw'. To create a clean dataset 'tr.blk', we filter trials to remove the first session (as it was a training session), session 41 (completed by only 1 patient), trials with score = 0, and trials marked bad by trainers.

```{r load_data, echo=FALSE, warning=FALSE}

pth <- '~/Dropbox/PROJECT_CENT/CENT_patients/CENT_DB_2013-08-27/Sessions/'

# Load the outcome and meta-data
nfb <- read.csv('test_outcomes.csv', sep="\t")
nfb <- nfb[order(nfb[,1]),]
mta <- read.csv(paste0(pth, 'meta_data.csv'), sep=",")
mta <- mta[order(mta[,1]),]

# Derive some indexing values from the datasets
n.NOR <- 38
n.INV <- 16
n.TRA <- 8
SBJS <- unique(nfb$Part_number)
n.SBJS <- NCOL(SBJS) * NROW(SBJS)
TB <- unique(subset(SBJS, nfb$TB.1.SMR1 == -1))
SMR <- unique(subset(SBJS, nfb$TB.1.SMR1 == 1))
MEN <- unique(subset(SBJS, nfb$Female0.Male2 == 2))
WMN <- unique(subset(SBJS, nfb$Female0.Male2 == 0))
DV <- "adj_score"
corvar <- 'secs'

# Get the most important data: trials/blocks, and make needed variables
tr.raw <- read.csv(paste0(pth, 'auto_block.csv'), sep=";")
tr.blk <- droplevels(subset(tr.raw, session > 1 & session < 41 & score != 0 & trainer_says_no != 1 & patient %in% SBJS))
tr.blk <- tr.blk[order(tr.blk[,1]),]
tr.blk$secs <- as.numeric(strptime(tr.blk$date_time, format='%Y:%m:%d %H:%M:%S'))

# Print summary of ALL trials in ALL categories
printTrials(tr.raw, DV, TB, SMR)
printTrials(tr.blk, DV, TB, SMR)
```


## Training modes

We also subset scores according to the three training modes (normal, inverse, transfer), and the complement set (normal+transfer = not inverse). So there are five possible (clean) datasets:

* all trials
* normal trials
* not-inverse trials
* inverse trials
* transfer trials

To proceed with Part A, we consider only _not-inverse_ trials.

```{r parse trials data, echo=FALSE, warning=FALSE}
# Subset the block data for analysis. Patients may have different start sessions for inverse, transfer.
tr.not0 <- getNotInvTrials(tr.blk) # not-inverse trials only
# tr.not1 <- getNotInvTrials(tr.blk, n.NOR, 1) # Normal trials only, pad/trim, cut 1-trial sessions

# Print summary of ALL trials in ALL categories
printTrials(tr.not0, DV, TB, SMR)
```


## Filtering for correlation calculations

We come across our first significant problem. For calculating trial-wise correlations, we require 2 or more trials per session. Many sessions contain only one trial of a certain type, especially for transfer trials. We therefore create datasets that prune out the sessions with trials = 1.
We also face a constraint when we calculate the cosine similarity of trial-wise correlations with a hypothetical multi-phase learning curve: the number of sessions should be enough to accommodate the definition of the hypothetical curve (e.g. 3+ for Fitts' model).

```{r parse trials data no singletons, echo=FALSE, warning=FALSE}
tr.not <- getNotInvTrials(tr.blk, min_trial = 1) # not-inverse trials only, cut sessions with 1 trial

# Print summary of ALL trials in ALL categories with single-trial sessions removed
printTrials(tr.not, DV, TB, SMR)
```

## Data description methods
We try to describe two different aspects of learning on a per-session basis:
(1) _Magnitude_ - derived as score mean
    * We use outlier-resistant _geometric mean_ of trial-wise scores per session. For comparison here, I also show the median-derived session-wise scores. Both might be fine for calculating session LCs because both are robust to outliers. However for NFB, because we can't assume any model for performance because we don't theoretically know how it happens, we don't want to totally reject outliers: they might represent something important. Thus, for this report we use the geometric mean. We center and scale the mean to lie from -1 to 1.
(2) _Consistency_ - derived as score monotonicity
    * We use rank-order correlation of per-trial adjusted score with trial-order. We use Kendall rather than Spearman for rank correlation as it is recommended for low N - the interpretation remains the same. The outcome range is -1 to 1, where monotonic increase in score per session results in Kendall t=1, and monotonic decrease results in Kendall t=-1. Presumably, monotonic increase in performance scores is a positive sign for learning.

Group-descriptive stats and plots per subject follow below:


```{r parse session data, echo=FALSE, warning=FALSE}
# Session data composed of: centered to 0, scaled to [-1..1], geometric mean trial scores
tr.not.scr <- getTrialCSGmean(tr.not, DV, cs = 'none')
print("GEOMETRIC MEAN ADJ SCORE:") 
describe(colMeans(tr.not.scr[,-1], na.rm = TRUE))

# Session data composed of median of trial scores
summary_method = "median"
sn.not <- getSSN(tr.not0, DV, summary_method) # Not-Inverse trials only

# Print summary of session-wise median of trials in each category
printTrials(sn.not, DV, TB, SMR)

# Kendall correlations of per-trial adjusted score with order - Kendall is used for small N
tr.not.cor <- getTrialCors(tr.not, DV, corvar)
print("KENDALL CORRELATIONS:")
describe(colMeans(as.data.frame(tr.not.cor[,-1]), na.rm = TRUE))

# Gather some data in long format
scorelong <- gather(as.data.frame(tr.not.scr), session, adj_score, 2:40, convert = TRUE, factor_key=TRUE, na.rm = TRUE)
kndlcorlg <- gather(as.data.frame(tr.not.cor), session, kendall_cor, 2:40, convert = TRUE, factor_key=TRUE, na.rm = TRUE)
#converting to a data.table object
scorelong.dt <- data.table(scorelong)

# Exploration: facet plot of score and kendall features for each subject
ggplot(scorelong, aes(x=session, y=adj_score)) +
  geom_jitter(alpha=0.4) +
  geom_smooth(method="lm", formula=y~x, se=F, alpha=0.8, linetype="dashed", size = 0.5) +
  facet_wrap(~patient) +
  theme_bw() +
  labs(x = "session#", y = "adjusted score")
ggplot(kndlcorlg, aes(x=session, y=kendall_cor)) +
  geom_jitter(alpha=0.4) +
  geom_smooth(method="lm", formula=y~x, se=F, alpha=0.8, linetype="dashed", size = 0.5) +
  facet_wrap(~patient) +
  theme_bw() +
  labs(x = "session#", y = "adjusted score")
```

# LC Analysis report - Part A - Methods + Results

## RQ1 - overall performance __magnitude__
* Method - linear model of all trials: use R to replicate Edua's thesis, test model fit
* Test - significance of slope ('growth curve')

We will make a series of linear mixed-effects models:
* Unconditioned: no factor for sessions, this simply models the intercept per subject
* Fixed linear, random intercept: we allow the intercept to be random but fix a single group-wide slope
* Random linear, random intercept: we allow the slope to be random per subject

The idea is that we can measure the improvement in model-fit as we make the models more realistic, and visualise the fit in terms of rediduals. Thus, below we see summaries for each fitted model, and two plots. First, linear models are plotted for each subject (black), overlaid by the 'prototype' function for the model, i.e. _Intercept + βx_. Next, the residuals are plotted, showing how much variance is not accounted for by the model.

```{r RQ1, echo=FALSE, warning=FALSE}
# ---- UNCONDITIONAL MEANS MODEL - BASE COMPARISON MODEL ----
um.fit <- lme(fixed = adj_score ~ 1, 
              random = ~ 1|patient, 
              data = scorelong,
              na.action = na.exclude)
summary(um.fit)
scorelong$pred.um <- predict(um.fit)
scorelong$resid.um <- residuals(um.fit)
#plotting PREDICTED intraindividual change; overlay PROTOTYPE (average individual)
fun.um <- function(x) { #create the function for the prototype
  as.numeric(um.fit$coefficients$fixed) + 0*x
}
#add the prototype as an additional layer
ggplot(data = scorelong, aes(x = session, y = pred.um, group = patient)) +
  ggtitle("Unconditional Means Model") +
  geom_line() +
  xlab("session") + 
  ylab("PREDICTED NF Score") + ylim(0,50) +
  stat_function(fun=fun.um, color="red", size = 2)
#plotting RESIDUAL intraindividual change
ggplot(data = scorelong, aes(x = session, y = resid.um, group = patient)) +
  ggtitle("Unconditional Means Model") +
  geom_line() +
  xlab("session") + 
  ylab("RESIDUAL NF Score")

# ---- FIXED LINEAR RANDOM INTERCEPT GROWTH MODEL (SESSION AS TIME)
fl.ri.fit <- lme(fixed = adj_score ~ 1 + session, 
                 random = ~ 1|patient, 
                 data = scorelong,
                 na.action = na.exclude)
summary(fl.ri.fit)
scorelong$pred.fl.ri <- predict(fl.ri.fit)
scorelong$resid.fl.ri <- residuals(fl.ri.fit)
#Create a function for the prototype
fun.fl.ri <- function(x) {
  as.numeric(fl.ri.fit$coefficients$fixed[1]) + as.numeric(fl.ri.fit$coefficients$fixed[2])*x
}
#plotting PREDICTED intraindividual change
ggplot(data = scorelong, aes(x = session, y = pred.fl.ri, group = patient)) +
  ggtitle("Fixed Linear, Random Intercept") +
  #  geom_point() + 
  geom_line() +
  xlab("session") + 
  ylab("PREDICTED NF Score") + 
  stat_function(fun=fun.fl.ri, color="red", size = 2)
#plotting RESIDUAL intraindividual change
ggplot(data = scorelong, aes(x = session, y = resid.fl.ri, group = patient)) +
  ggtitle("Fixed Linear, Random Intercept") +
  #  geom_point() + 
  geom_line() +
  xlab("session") + 
  ylab("RESIDUAL NF Score")

# ---- RANDOM LINEAR SLOPES AND INTERCEPTS ----
rl.ri.fit <- lme(fixed = adj_score ~ 1 + session,
                 random = ~ 1 + session|patient,
                 data = scorelong,
                 na.action = na.exclude)
summary(rl.ri.fit)
# intervals(rl.ri.fit)
#Place individual predictions and residuals into the dataframe
scorelong$pred.rl.ri <- predict(rl.ri.fit)
scorelong$resid.rl.ri <- residuals(rl.ri.fit)
#Create a function for the prototype
fun.rl.ri <- function(x) {
  as.numeric(rl.ri.fit$coefficients$fixed[1]) + as.numeric(rl.ri.fit$coefficients$fixed[2])*x
}
#plotting PREDICTED intraindividual change
ggplot(data = scorelong, aes(x = session, y = pred.rl.ri, group = patient)) +
  ggtitle("Random Linear, Random Intercept") +
  #  geom_point() + 
  geom_line() +
  xlab("session") + 
  ylab("PREDICTED NF Score") +
  stat_function(fun=fun.rl.ri, color="red", size = 2)
#plotting RESIDUAL intraindividual change
ggplot(data = scorelong, aes(x = session, y = resid.rl.ri, group = patient)) +
  ggtitle("Random Linear, Random Intercept") +
  #  geom_point() + 
  geom_line() +
  xlab("session") + 
  ylab("RESIDUAL NF Score")
```

### RQ 1 testing

We can then test the various models by ANOVA, to check the significance of model fit differences.

```{r RQ1 test, echo=FALSE, warning=FALSE}
# significance of random slopes: compare models by anova() for difference in fit between two nested models
anova(um.fit,fl.ri.fit)
anova(fl.ri.fit,rl.ri.fit)
```



## RQ2 - plateau of performance improvement reached?
* Method - quadratic model of all trials, fit quadratics and estimate proportion of positive vs negative signs
* Test - check sign of each quadratic curve, positive = U-shaped with no plateau, negative = ∩-shaped with plateau

First, we will visualise this concept with a quadratic function fitted to data for each subject: _y = β.session² + β.session + E_

The plot is sorted from top left by protocol and gender. The second-order coefficient of this quadratic function, i.e. _β.session²_, expresses the concept of __plateau__ in the data in its sign. The sign is positive if the curve is concave (bending up at the ends, u-shaped), or negative if convex (bending down at the ends, n-shaped). A convex curve has a plateau, a concave doesn't.

```{r RQ2, echo=FALSE, warning=FALSE}
## ---- Method - quadratic fit for each subject
nfOrd <- setorder(data.table(nfb), TB.1.SMR1, Female0.Male2)
subOrd <- nfOrd$Part_number
quads <- matrix(NaN, nrow=n.SBJS, ncol=39)
quad.lms <- list()
quad.x2s <- vector(length = n.SBJS)
rownames(quads) <- subOrd
sex = c("F", "M")
protocol = c("TB","null", "SMR")
# subplots for each participant, from trial scores
par(mfrow=c(6,4),mar=c(1,2,1,1))
for (i in seq_along(subOrd)){
  #data prep
  p <- subOrd[i]
  tmp <- tr.not #outgoners(tr.not, tr.not$adj_score, thr=3)
  tmp <- ddply(subset(tmp, patient==p), .(session), summarize, adj_score=mean(adj_score))
  #model fit
  tmp.lm <- with(tmp, lm(adj_score ~ session + I(session^2)))
  quad.lms[[i]] <- tidy(tmp.lm)
  quad.x2s[i] <- as.numeric(tmp.lm$coefficients[3])
  quads[i,1:nrow(tmp)] <- fitted(tmp.lm)
  #plotting
  sgn = "-"
  if (quad.x2s[i] > 0){
    sgn = "+"
  }
  ttl = paste0(p, ", ", sex[nfOrd[subOrd==p,]$Gender..1.F..2.M.], ", ", protocol[nfOrd[subOrd==p,]$TB.1.SMR1 + 2], ", ", sgn)
  if (i < length(subOrd) - 3){
    plot(tmp, main=ttl, xlab="", ylab="", col=2, xaxt='n')
  }else{
    plot(tmp, main=ttl, xlab="", ylab="", col=2)
  }
  lines(na.omit(quads[i, 1:nrow(tmp)]), col=3, lwd=2)
}
plot.new()
par(mfrow=c(1,1))
names(quad.x2s) <- subOrd
```

### RQ2 - testing 1
We can compare the 2nd-order coefficient sign of each subject with their session-coefficient from the random slopes and intercepts growth model above, which captures the degree of learning. This is done by Pearson correlation, reported below.

```{r RQ2 testing 1, echo=FALSE, warning=FALSE}
# compare fitted model curve direction to learning, i.e. PLATEAU v LEARN
LINEAR_LEARNING_COEF <- rl.ri.fit$coefficients$random$patient[,2]
QUADRATIC_SIGN <- quad.x2s[sort(names(quad.x2s))]
cor.test(LINEAR_LEARNING_COEF,QUADRATIC_SIGN)
```

### RQ2 - quadratic growth model

Next, we extend the growth modelling approach by adding a fixed effect for the square of the session. We visualise this model in the same way as before.

```{r RQ2 quad growth, echo=FALSE, warning=FALSE}
## ---- Method - quadratic growth model of all subjects
rq.ri.fit <- lme(fixed = adj_score ~ 1 + session + I(session^2),
                 random = ~ 1 + session|patient,
                 data = scorelong,
                 na.action = na.exclude)
summary(rq.ri.fit)
#Place individual predictions and residuals into the dataframe
scorelong$pred.rq.ri <- predict(rq.ri.fit)
scorelong$resid.rq.ri <- residuals(rq.ri.fit)
#Create a function for the prototype
fun.rq.ri <- function(x) {
  as.numeric(rq.ri.fit$coefficients$fixed[1]) + 
    as.numeric(rq.ri.fit$coefficients$fixed[2])*x + 
    as.numeric(rq.ri.fit$coefficients$fixed[3])*x^2
}
#plotting PREDICTED intraindividual change
ggplot(data = scorelong, aes(x = session, y = pred.rq.ri, group = patient)) +
  ggtitle("Random Quadratic, Random Intercept") +
  geom_line() +
  xlab("session") + 
  ylab("PREDICTED NF Score") +
  stat_function(fun=fun.rq.ri, color="red", size = 2)
#plotting RESIDUAL intraindividual change
ggplot(data = scorelong, aes(x = session, y = resid.rq.ri, group = patient)) +
  ggtitle("Random Quadratic, Random Intercept") +
  geom_line() +
  xlab("session") + 
  ylab("RESIDUAL NF Score")
```

### RQ2 - testing 2
We can also test the quadratic fit growth model against the earlier linear model using ANOVA.

```{r RQ2 testing 2, echo=FALSE, warning=FALSE}
# test quadratic fit growth model against earlier linear model using ANOVA.
anova(rl.ri.fit,rq.ri.fit)
```


##  RQ3 - what kind of learning is seen, in terms of curve families, e.g. power law or exponential?
* Method - fit curves from separate families to data, test/compare model fit
    * power law: linear in log-log space
    * exponential: linear in semi-log space
* Tests - transform curves to a linear space to test fit with r²

We can view the outcome of fitting a power law curve to our data by examining a linear fit in the log-log transformation space: i.e. the log-transform of both data dimensions (score & session). We can further fit a linear growth model to log-log data to find the quality of fit of a power law.
For the exponential curve, we simply repeat the process in log-linear space, i.e. log transform score but leave session as is.

```{r RQ3, echo=FALSE, warning=FALSE}
# PLOT in log-log space
ggplot(scorelong, aes(x=log(session), y=log(adj_score))) + #, color=hrs_since_sleep)) + 
  geom_jitter(alpha=0.4) +
  geom_smooth(method="lm", formula=y~x, se=F, alpha=0.8, linetype="dashed", size = 0.5) +
  # scale_color_gradient(low="green", high="red") +
  facet_wrap(~patient) +
  theme_bw() +
  labs(x = "log(session#)", y = "log(adjusted score)") #, color = "hours awake")
# THEN, fit a linear model to all patients (so to compare curves with ANOVA)
loglog.rl.ri.fit <- lme(fixed = log(adj_score) ~ 1 + log(session),
                    random = ~ 1 + log(session)|patient,
                    data = scorelong,
                    na.action = na.exclude)
summary(loglog.rl.ri.fit)

# * EXPONENTIAL: linear in semi-log space
ggplot(scorelong, aes(x=session, y=log(adj_score))) + #, color=hrs_since_sleep)) + 
  geom_jitter(alpha=0.4) +
  geom_smooth(method="lm", formula=y~x, se=F, alpha=0.8, linetype="dashed", size = 0.5) +
  facet_wrap(~patient) +
  theme_bw() +
  labs(x = "session#", y = "log(adjusted score)")
# THEN, fit a linear model to all patients (so to compare curves with ANOVA)
loglin.rl.ri.fit <- lme(fixed = log(adj_score) ~ 1 + session,
                        random = ~ 1 + session|patient,
                        data = scorelong,
                        na.action = na.exclude)
summary(loglin.rl.ri.fit)
```

### RQ3 - testing
We then compare the fit quality of these models; it doesn't make sense using ANOVA, because the fixed effects change, but we can observe the differences in model fit indices AIC or BIC. In the same way, we can also compare the best fitting curve to the best linear model, above.

```{r RQ3 testing, echo=FALSE, warning=FALSE}
# * Tests - measure fit of linear regression with r²
prettyPrintModelFit("Random slope+intercept linear model:", rl.ri.fit)
prettyPrintModelFit("Random slope+intercept log-log model:", loglog.rl.ri.fit)
prettyPrintModelFit("Random slope+intercept log-linear model:", loglin.rl.ri.fit)
```



## RQ4 - Is the skill acquisition theory supported by our novel non-parametric model?
* Method - fit the session-wise magnitude scores (geometric mean) and consistency index (Kendall correlation) to models of possible learning trajectories: monotonic, Fitts-Posner; with following fitting methods:
    * Kendall correlation: across-sessions correlation
    * Cosine similarity: across-sessions custom profile of scores/change indices.
* Tests - test significance of distribution and model fit
    * difference of cosine value distribution mean from 0
    * direct estimate of fit from Kendall correlation/cosine similarity

Given LCs based on the two types of session-performance index, _magnitude_ and _consistency_, we want to establish if they display a pattern that matches skill acquisition theory. The hypothetical skill acquisition learning curve follows the Fitts-Posner three stage model (REF - see Edua's theory text 07.06.2017).

We fit our data to this model by taking the cosine similarity of each subject's LC with a canonical LC that represents Fitts' model: this is our __ideal LC__. We can test if the resulting distribution mean differs from 0 for the group, in order to determine if this model captures the learning that we know has occured.

We also want a comparison model, to determine whether our Fitts model fits the data better than some simpler explanation. Cosine similarity ranges from 1 to -1, as does correlation. Thus we can use Kendall correlation across sessions for score _magnitude_, to derive a __monotonic LC__. We can do similarly for _consistency_, by aggregating the per-session correlations to derive a __consistent LC__ (using Hunter-Schmidt method, see Zhang & Wang (2014), Multivariate Behavioral Research, 49:2, 130-148).

Fitts' model has three phases. Our choice of Fitts' model is initially (0, 1, 0.5). The choice of model impacts the testing outcomes very strongly and will have to be explored in more detail later.

```{r RQ4, echo=FALSE, warning=FALSE}
## ## ---- CALCULATE NON-PARAMETRIC LCs --------------------

## ## ---- CONSISTENCY of LEARNING ----
# Aggregation of Kendall correlations of per-trial adjusted score with order
ConsLC.not <- getTrialAggCor(tr.not, DV, corvar)

## ## ---- MONOTONICITY of LEARNING ----
# Kendall correlation of per-session geometric mean score with session number
MonoLC.not <- getSsnMLC(scorelong, DV, precomp = TRUE)
# LC <- setDT(scorelong)[
#     , .(cors = cor(adj_score, session, use="pair", method="k"))
#     , by = patient]$cors

## ## ---- IDEAL LEARNING ----
# cosine similarity of trial cors to an arbitrary hypothetical LC (AHLC)
FITTs <- c(0, 1, 0.5)
print(paste("Fitt's model for monotone improvement = ", toString(FITTs)))
IcorLC.not <- getTrialCosSim(tr.not.cor[,-(1)], tr.not.cor[,1], AHLC = "Fitts", phase123 = FITTs)
# is this defined also for geometric mean scores?

# Fitts model for geometric mean score
FITTs <- c(-0.5, 0, 0.5)
print(paste("Fitt's model for geometric mean score = ", toString(FITTs)))
# cosine similarity of trial scores to an arbitrary hypothetical LC (AHLC)
IscrLC.not <- getTrialCosSim(tr.not.scr[,-(1)], tr.not.scr[,1], AHLC = "Fitts", phase123 = FITTs)
```

"MonoLC.not" = monotonic LC (correlation across session means) for NOT-INVERSE trials data:
```{r RQ4 not-inverse trials MLC, echo=FALSE, warning=FALSE}
summary(MonoLC.not$ssn.MLC)
```

"ConsLC.not" = consistency LC (aggregate of session-wise correlations) for NOT-INVERSE trials data:
```{r RQ4 not-inverse trials CLC, echo=FALSE, warning=FALSE}
summary(ConsLC.not$agg.cor)
```

"IscrLC.not" = per-session geometric mean score cosine similarity LC for NOT-INVERSE trials data:
```{r RQ4 not-inverse trials Iscore LC, echo=FALSE, warning=FALSE}
summary(IscrLC.not[,2])
```

"IcorLC.not" = per-session correlations cosine similarity LC for NOT-INVERSE trials data:
```{r RQ4 not-inverse trials Icorrelation LC, echo=FALSE, warning=FALSE}
summary(IcorLC.not[,2])
```

### RQ4 - Examine LCs by plotting

It is useful to look at the LC data in raw form. The _consistency_, _monotonic_ and _ideal_ LCs are real-valued, have the same range [-1..1], and similar interpretation (-1..1 = distance from perfect performance according to the model being used). Thus, we examine the four LCs side-by-side in the same plot. 

```{r RQ4 plot LC, echo=FALSE, warning=FALSE}
df <- as.data.frame(cbind(MonoLC.not[,2], IscrLC.not[,2], 
                          ConsLC.not[,2], IcorLC.not[,2]))
names(df) <- c("MonoLC", "IdealLC.scor", "ConsLC", "IdealLC.cor")
ggplot(melt(df), aes(factor(variable), value)) + geom_boxplot() + 
  geom_point(aes(color = factor(variable)), position = position_dodge(width = 0.5))
```

We can also sort the main LCs and view scatterplots to get a sense of how clustered the results are. 

```{r RQ4 LC scatters, echo=FALSE, warning=FALSE}
par(mfrow = c(1,4))
plot(sort(MonoLC.not[,2]), ylab = "sorted LC score", main = "MonoLC")
abline(h = 0.5, lty = 3)
plot(sort(ConsLC.not[,2]), ylab = "", main = "ConsLC")
abline(h = 0.5, lty = 3)
plot(sort(IscrLC.not[,2]), ylab = "", main = "IdealLC.score")
abline(h = 0.5, lty = 3)
plot(sort(IcorLC.not[,2]), ylab = "", main = "IdealLC.correl")
abline(h = 0.5, lty = 3)
par(mfrow = c(1,1))
```

We can explore how the LCs relate to each other using a correlation matrix plot. Variables with their ranges lie on the diagonal. Over the diagonal are correlations of variable-pairs, and confidence intervals (in parentheses). Under the diagonal are loess-curve fits to the scatter plots of variable-pairs.

```{r RQ4 LC correl matrix, echo=FALSE, warning=FALSE}
corrMatrix("MonoLC.not", MonoLC.not[,2],
           "ConsLC.not", ConsLC.not[,2], 
           "IscrLC.not", IscrLC.not[,2],
           "IcorLC.not", IcorLC.not[,2],
           title = "Monotonic, Consistency, Ideal.score, Ideal.correl LCs")

```


### RQ4 - Testing LCs

First, we want to know whether each LC distribution was significantly biased, i.e. does the LC represent a consistent pattern of change across the group (do they improve)? 

The _monotonic_ and _consistency_ LCs are based on calculations of Kendall's tau. Under the null hypothesis of independence of X and Y, the sampling distribution of tau has an expected value of zero. Thus, group-wise null hypothesis can be that the distribution of LCs has a mean value close to zero. Because we have a small sample, we will not assume that the LC has a normal distribution, but instead use Wilcoxon's non-parametric test.

Similarly using Wilcoxon's non-parametric test, we also test whether the group-wise Ideal LC distribution was significantly biased, i.e. do they learn a skill? 

```{r RQ4 test LC significance, echo=FALSE, warning=FALSE}
wilcox.test(MonoLC.not[,2], exact = TRUE, conf.int = TRUE)
wilcox.test(ConsLC.not[,2], exact = TRUE, conf.int = TRUE)

wilcox.test(IscrLC.not[,2], exact = TRUE, conf.int = TRUE)
wilcox.test(IcorLC.not[,2], exact = TRUE, conf.int = TRUE)
#  ₀ ₁ ₂ ₃ ₄ ₅ ₆ ₇ ₈ ₉ ₊ ₋ ₌ ₍ ₎ ₐ ₑ ₕ ᵢ ⱼ ₖ ₗ ₘ ₙ ₒ ₚ ᵣ ₛ ₜ ᵤ ᵥ ₓ ₔ. 
# ⁰ ¹ ² ³ ⁴ ⁵ ⁶ ⁷ ⁸ ⁹ ⁺ ⁻ ⁼ ⁽ ⁾ ⁿ ⁱ
```

Finally, the idea of including monotonic and consistency LCs is that they provide a simple base case to compare the Ideal LCs. We do this by comparing how well they fit the data. Since all LCs share the same distribution ranges, they can all be subjected to the same logic: i.e. a perfect relation between model and data would give a score of ±1, and an orthogonal relation gives 0. Thus the model fit can be calculated simply by _Σ₁ⁿ 1 - |xₙ| / n_, i.e. the average difference of absolute values from 1. _As usual with fit indices, lower is better!_

```{r RQ4 compare LC fits, echo=FALSE, warning=FALSE}
# Fit of the chosen Fitts-Posner learning vector (0, 1, 0.5) to the Kendall-correlations, by cosine similarity
CosSimFit <- function(x){
  # Sigma_(1..n) 1 - |x_n| / n 
  mean(1 - abs(na.omit(x)))
}
cat("MonoLC model fit: ", CosSimFit(MonoLC.not[,2]))
cat("ConsLC model fit: ", CosSimFit(ConsLC.not[,2]))
cat("IdealLC.score model fit: ", CosSimFit(IscrLC.not[,2]))
cat("IdealLC.correl model fit: ", CosSimFit(IcorLC.not[,2]))
```


# Part A - DISCUSSION

## RQ1
For RQ1, we see that the random slopes and intercepts model has the best fit, significant by ANOVA at _p<.0001_. The group-level coefficient of session is ~0.16, giving a total raise of ~6.08 over the measured 38 sessions, significant at p<0.00001.

## RQ2
For RQ2, we see that most subjects (n=15) have a plateau, i.e. quadratic is convex. The minority with no plateau (concave curves, n=8) is big enough to be meainingful though.  The Pearson's correlation between them is r=-0.44, significant at p<0.05. Since negative signs indicate convex/plateaued curves, the negative correlation indicates that subjects who learned more had a plateau. 

Interestingly, the quadratic growth model doesn't capture the concavity of some data (all individual curves are convex in the plot); it also doesn't fit the data any better than the linear model. Thus it is probably the wrong way to approach this question.

## RQ3
The subject-wise plots show that a power law AND an exponential curve fit very well to the data (data are almost linear in the transform space), for quite a few subjects, but the fit is not good for a substantial minority. The comparison of fitting indices shows that the two curve families are relatively equal: power law/log-log is slightly better than exponential/log-linear. However both are an order of magnitude better than the linear model, showing that even if these curves do not fit perfectly for everyone (motivating the non-parametric approach), the majority pattern is that subjects learn in a classic 'power-law' way.

## RQ4

There quite a few results here: 
* The boxplot and Wilcoxon tests show that all LCs capture some kind of positive relationship: all are significantly different to zero. 
* The correlation matrix indicates that LCs that are based on the same data (session-wise correlations or mean scores) are quite highly correlated (>0.7).
* The scatter plots show that each LC is distributed quite uniformly across the range: no strong clustering.
* The fit values are all quite poor, but especially for ConsLC (0.87) and IdealLC.score (0.92). 
* Finally, the comparison of LCs by fit indicates: (a) IdealLC.score is a poorly chosen model compared with MonoLC; and (b) IdealLC.correl improves over the base model ConsLC.


## Further work?

RQ1-3 seem like good and complete analyses, from which insights and reporting can be drawn. RQ4 still seems to lack a major insight: it would be great to try improving the model fitting scores by optimisation; however this might not be possible in the near future.


# LC Analysis report - Part B - Methods + Results

## Part B - Data preparation
__TODO__: _describe data for Part B RQs_

For each dataset we create a new index of sessions so that rows can be aligned according to how many sessions of a protocol were conducted, as opposed to what the session number was when starting. Thus, e.g. transfer trials are indexed from 1 to 10 for all subjects, regardless of what session it was that their transfer trials really started (first transfer trial ranged from session 28 to session 35, depending on subject).

...

However, pruning out sessions with 1 trial results in losing quite a lot of data. 

...

The total number of sessions per subject for each training mode now varies quite a bit: this is no problem because our LC calculation methods are not sensitive to small differences in N, except when N is very small (see more below). However, for the transfer training mode, removing sessions with 1 trial results in losing entire subjects. For this reason, because parametric or session-wise LCs are measuring a different thing to trial-wise correlation-based LCs, we can use different datasets for each: for session LCs, we will include sessions with 1 trial.

...

We can further subdivide data by subjects in TB protocol and subjects in SMR protocol...

