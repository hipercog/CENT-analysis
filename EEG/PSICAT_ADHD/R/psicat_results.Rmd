---
title: "R Notebook"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
    theme: united
---

```{r message=F}
library(car)
library(MASS)
library(data.table)
library(tidyverse)
library(magrittr)
# library(Rtools)
library(retimes)
library(boot)
library(zoo)
library(broom.mixed)
library(lme4)
library(lmerTest)
library(sjPlot)
library(emmeans)
library(here)

#behavioural data: RT and error rates
rtimes <- readRDS(file.path(here(), 'data', 'rtimes_errors.rds'))
error_rates <- readRDS(file.path(here(), 'data', 'error_rates.rds'))

#neural data
df <- readRDS(file.path(here(), 'data', 'psicat_neural.rds'))

#set contrast options
options(contrasts = c("contr.sum","contr.poly"))
```

# Behavioural results

Trial-wise *reaction times* are used to find subject-level parameters of ex-Gaussian distributions. These parameters are then used as DVs in LMMs. Subject-level *error rates* are DVs in logistic regression (GLMM).  

## Ex-Gaussian distribution parameters  
Estimating mu, sigma, tau for each participant and condition.  
**mu**: mean of Gaussian distribution  
**sigma**: SD of Gaussian distribution  
**tau**: tail (greater tau -> more long RTs -> positive skew)  

```{r}
sbj_params_ml <- rtimes %>%
  filter(TTime > 100, error == 0) %>% #RT > 100, no errors
  group_by(group, Subject, shape, congruency) %>% 
  summarise(timefit_out = list(data.frame(t(attr(retimes::timefit(TTime),"par"))))) %>%
                   unnest(cols = c(timefit_out)) %>% ungroup
```

### Plot parameter distributions
```{r fig.width=8, fig.height=8}
sbj_params_ml %>%
  gather(var, value, mu:tau) %>%
  ggplot(aes(value, fill=interaction(congruency, shape))) + 
  geom_density(alpha=.4) + 
  facet_wrap(var~group, ncol=2) + theme_bw()
```

## Fitting lmer with ex-Gaussian parameters 
```{r}
sbj_params_ml <- sbj_params_ml %>%
  mutate(group = relevel(factor(group), ref = "ctrl"),
         congruency = relevel(factor(congruency), ref = "Congruent"),
         shape = relevel(factor(shape), ref = "shape"))

```
### mu
```{r}
mmu <- lmer(mu ~ group*shape*congruency + (1|shape:Subject) + (1|congruency:Subject) + (1|Subject), data=sbj_params_ml)
summary(mmu)
plot_model(mmu, type = "diag")[[4]]
```
### sigma
**Note warning: singular fit**  
```{r}
msig <- lmer(sigma ~ group*shape*congruency + (1|shape:Subject) + (1|congruency:Subject) + (1|Subject), data=sbj_params_ml)
summary(msig)
#plot_model(msig, type = "diag")[[4]]

```

### tau
```{r}
mtau <- lmer(tau ~ group*shape*congruency + (1|shape:Subject) + (1|congruency:Subject) + (1|Subject), data=sbj_params_ml)
summary(mtau)
#plot_model(mtau, type = "diag")[[4]]

```

### tau (log model)
```{r}
mtau.log <- lmer(log(tau) ~ group*shape*congruency + (1|shape:Subject) + (1|congruency:Subject) + (1|Subject), data=sbj_params_ml)
summary(mtau.log)
#plot_model(mtau.log, type = "diag")[[4]]
```


### Plot fixed effects
```{r}
new_data <- expand.grid(shape = c('shape', 'nonShape'), 
                      congruency = c('Congruent', 'InCon'), 
                      group = c('adhd', 'ctrl'))
                      
models <- list(mmu = mmu, msig = msig, mtau = mtau, mtau.log = mtau.log)

p <- lapply(seq_along(models), function(i) {
  cbind(new_data, rtime = predict(models[[i]], newdata = new_data, re.form=NA)) %>%
  ggplot(aes(congruency, rtime, linetype=shape)) + geom_point() + 
  geom_line(aes(group=shape),size=1) +
  facet_wrap(~group) + theme_bw() + ggtitle(names(models)[i])
})

p
```

### Contrasts
So far for mtau.log model only.  
**TODO**: Which are relevant for RQs?  
```{r}
means.int <- emmeans(mtau.log, specs = c("group", "shape", "congruency")) 
means.int
pairs(means.int) #all pairs

cont <- contrast(means.int, list(shape_nonshape = c(1,-1)), by=c("congruency", "group")) #Estimate is the same as from effects library, but t value is used instead of z
#contrast(means.int, list(shape_nonshape = c(1,-1)), simple="shape") # this is the same thing; either specify simple or by
#coef(cont)
cont

#also same as:
#means.int <- emmeans(mtau.log, specs = c("group", "shape", "congruency"), by = c("congruency", "group"))
#cont <- contrast(means.int, list(shape_nonshape = c(1,-1))) # OR pairs(means.int)

#or:
#lsm <- emmeans(mtau.log, ~shape|congruency:group) #shape contrasts at each level of congruency and group
#cont <- contrast(lsm, interaction = "pairwise") # OR pairs(lsm)


```

```{r}

emms1 <- emmeans(mtau.log, ~ shape*congruency|group) #shape-congruency interaction for each group
#emmip(mtau.log, shape ~ congruency | group, CIs = T)
con1 <- contrast(emms1, interaction = "pairwise") #interaction effects for each group. Tukey adjustment by default; same as interaction contrasts from joint_tests (below)
pairs(con1, by = NULL) #compare interaction effects pairwise (between groups). same as group:shape:congruency joint_tests without by parameter 

```

```{r}
#interaction contrasts for all effects
joint_tests(mtau.log, by = "group")
joint_tests(mtau.log) #same as fixed effects of lmer summary
```


### Bootstrapping lmer estimates

```{r}
bootstrap.estimates <- function(model) {
  cb <- bootMer(model, FUN=fixef, nsim=1000 #.progress = "txt", PBargs = list(style=3)
                )
  cb
}

bootstrap.estimate.plot <- function(cb, model) {
  boot_ci <- lapply(2:8, function(b) boot.ci(cb, type = "perc", index=b)$percent)

  ci <- do.call(rbind, boot_ci)
  td <- broom.mixed::tidy(model)[2:8,]  
  td$conf.low <- ci[, 4]
  td$conf.high <- ci[, 5]
  g <- ggplot(td, aes(term, estimate)) +
    geom_point() +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width=.1) + coord_flip() +
    geom_hline(aes(yintercept=0), linetype= "longdash") + theme_bw()
  g
}

```


```{r}
b.models <- lapply(models, bootstrap.estimates)

lapply(seq_along(models), function(i) {
  p <- bootstrap.estimate.plot(b.models[[i]], models[[i]])
  p + ggtitle(names(models)[i])
})

```


## Error rate analysis

Using glmer logistic regression with error rate as DV and number of trials as weights.  
(with optimizer to solve convergence issue; producing very similar results but without warnings)  
```{r}
glmm.fit <- glmer(err ~ group*shape*congruency + (1|shape:Subject) + (1|congruency:Subject) + (1|Subject), family = binomial(logit), data=error_rates, weights=n, 
                  glmerControl(optimizer = 'nlminbwrap'))
summary(glmm.fit)

# help("convergence")
# glmm.all <-  allFit(glmm.fit) #try all available optimizers to solve convergence issue
# ss <- summary(glmm.all)

glmm.probs <- cbind(error_rates, pred = predict(glmm.fit, type = "response"))

glmm.probs %>%
  gather(var, value, err, pred) %>%
  unite(cond, congruency, shape) %>%
  ggplot(aes(cond, value, color=var)) + geom_boxplot() + facet_wrap(~group) +
  theme_bw() + ggtitle('Actual and predicted error rates') +
  theme(axis.text.x = element_text(angle = 15)) 


```

**TODO**: Post-hoc tests, diagnostics, bootstrapping?   

## Trial-wise RTs over time
Note: some subjects have a LOT of errors (black dots), e.g. 1057P.  
```{r fig.height=100, fig.width=10}
rtimes %>%
  mutate(error_hl = ifelse(error == 1, TTime, NA_real_)) %>%
  ggplot(aes(Trial, TTime, color=group, group=Subject)) + geom_line(size=1) + theme_bw() +
  geom_point(aes(Trial, error_hl), col="black", size=.8) +
  facet_wrap(~Subject, ncol=1)

```
# Neural results: ERPs

Subject 2011C excluded because of bad data.  

(N1: occipital, P3a: vertex, P3b: parietal)  
H3P3a, H3P3b, H4pN1 with baseline -250...-150  
H4tN1, H4tP3 with baseline -250...0  

Questions of congruency: pre-primer baseline   
Questions of shape: separate baselines   

H3 (congruency):  
P3a: baseline -250...-150, latency from target  --> "P3aprimer"  
P3b: baseline -250...-150, latency from target  --> "P3b"  

H4 (shape):  
N1 primer: baseline -250...-150, latency from primer   --> "N1primer"  
N1 target: baseline -250...0, latency from target  --> "N1target"  (in each congruency condition)  
P3a: baseline -250...0, latency from target --> "P3atarget"  (in each congruency condition)  

(Note "P3aprimer" can be a misleading name because latency is calculated from target onset.)  

## Local peaks within windows
Latencies of local minima for each subject and shape condition within -50...50 ms are searched (see **N1primer figures** for example). Then, subject-wise windows with a width corresponding to the assumed component duration (40 ms?) can be centered on these latencies, and mean amplitude calculated from that window.    
```{r, warning=F, echo=F}
df %>%
  filter(sbj != '2011C') -> df

#function for plotting subject-wise figures

#define initial windows here
vlines <- list(N1primer = data.frame(xint = c(-50, 50), roi = c('occipital'), pritar = 'primer'),
               N1target = data.frame(xint = c(100, 200), roi = c('occipital'), pritar = 'target'),
               P3aprimer = data.frame(xint = c(200, 300), roi = c('vertex'), pritar = 'primer'),
               P3atarget = data.frame(xint = c(200, 300), roi = c('vertex'), pritar = 'target'),
               P3b = data.frame(xint = c(300, 500), roi = c('parietal'), pritar = 'primer'))

vlines <- lapply(vlines, cbind, grp = 'bound')
vlines <- lapply(vlines, rbind, data.frame(xint = 0, roi = NA_character_, pritar = NA_character_, grp = 'zero')) #add zero and factors for plotting purposes

sbjs <- df %>% distinct(sbj) %>% pull()

plot_sbj_erps <- function(component, ...) {
  vline_plot <- vlines[[deparse(substitute(component))]]  
  group_var <- enquos(...)
  compname <- deparse(substitute(component))
  
  if (startsWith(deparse(substitute(component)), 'N')) func <- which.min
  else func <- which.max
  

sapply(sbjs, function(x) {
  
  p <- df %>%
  filter(roi == vline_plot$roi[1], 
         pritar == vline_plot$pritar[1],
         sbj == x) %>%
  group_by(!!! group_var, time) %>%
  summarise(mean = mean(value)) %>%
  arrange(!!! group_var, time) %>%
  group_by(!!! group_var) %>%
  mutate(roll =  rollapply(as.zoo(mean), 3, function(x) func(x)==2, fill=NA)) %>%
  group_by(window = between(time, vline_plot[1,1], vline_plot[2,1]), !!! group_var) %>%
  mutate(erp = ifelse(window == T & roll == T, mean, NA_real_)) %>%
    ggplot(aes(time, mean, color=interaction(!!! group_var), group = interaction(!!! group_var))) + 
    geom_line() + 
    geom_point(aes(time, erp, fill=interaction(!!! group_var)), colour="black", pch=21, size=3, alpha=.7) +
    scale_x_continuous(breaks = seq(-250,500, by=50)) +
#   scale_y_reverse() + 
    geom_vline(data = vline_plot, aes(xintercept = xint, linetype = grp), alpha=.7, show.legend = F) +  theme_bw() 

  ggsave(paste0("subject_erp_figs/", compname, '_', x, ".png"), 
         plot = p, width = 30, height = 20, units = "cm")
  }
  )
}

plot_sbj_erps(N1primer, shape)

```

 
If multiple peaks are found, we need a decision rule:    
- choose latency of the most negative amplitude?  
- choose latency that captures both shape and nonshape peaks?  
- something else?  

```{r warning=F}
#Peak amplitude and latency values for N1primer. 
df %>%
  filter(roi == 'occipital', 
         pritar == 'primer') %>%
  group_by(group, sbj, shape, time) %>%
  summarise(mean = mean(value)) %>%
  arrange(group, sbj, shape, time) %>%
  group_by(group, sbj, shape) %>%
  mutate(roll =  rollapply(as.zoo(mean), 3, function(x) which.min(x)==2, fill=NA)) %>%
  filter(between(time, -50, 50), roll==T) 

```

**Another (simpler) option would be to use a fixed window that is wide enough (100ms?) and calculate mean amplitude from that, instead of using subject-wise windows.**   

#### Testing with N1primer

Taking the most negative amplitude and calculating mean amplitudes for N1primer, with window widths 40ms and 100ms:
```{r warning=F}
df %>%
  filter(roi == 'occipital', 
         pritar == 'primer') %>%
  group_by(group, sbj, shape, time) %>%
  summarise(mean = mean(value)) %>%
  arrange(group, sbj, shape, time) %>%
  group_by(group, sbj, shape) %>%
  mutate(roll =  rollapply(as.zoo(mean), 3, function(x) which.min(x)==2, fill=NA)) %>% 
  filter(between(time, -50, 50), roll==T) %>%
  group_by(group, sbj) %>%
  filter(mean == min(mean,na.rm=T)) %>%
  rename(peak_lat = time) %>%
  ungroup %>%
  select(group, sbj, peak_lat) -> N1primer_times
  
df %>%
  filter(roi == 'occipital', 
         pritar == 'primer') %>%
  group_by(group, sbj, shape, time) %>%
  summarise(mean = mean(value)) %>%
  left_join(N1primer_times, by = c('group','sbj')) %>%
  filter((time > peak_lat - 20) & (time < peak_lat + 20)) %>%
  group_by(group, sbj, shape) %>%
  summarise(mean_amp = mean(mean)) %>%
  spread(shape, mean_amp) %>%
  mutate(shape_diff = nonshape - shape) %>%
  gather(var, value, nonshape:shape_diff) %>%
  ggplot(aes(group, value)) + geom_violin(alpha=.4) + facet_wrap(~var) + theme_bw() +
  ggtitle('40ms window') +
  scale_y_continuous(limits = c(-10, 10))

df %>%
  filter(roi == 'occipital', 
         pritar == 'primer') %>%
  group_by(group, sbj, shape, time) %>%
  summarise(mean = mean(value)) %>%
  left_join(N1primer_times, by = c('group','sbj')) %>%
  filter((time > peak_lat - 50) & (time < peak_lat + 50)) %>%
  group_by(group, sbj, shape) %>%
  summarise(mean_amp = mean(mean)) %>%
  spread(shape, mean_amp) %>%
  mutate(shape_diff = nonshape - shape) %>%
  gather(var, value, nonshape:shape_diff) %>%
  ggplot(aes(group, value)) + geom_violin(alpha=.4) + facet_wrap(~var) + theme_bw() +
  ggtitle('100ms window') +
  scale_y_continuous(limits = c(-10, 10))

```
Using fixed window of 100ms (-50...50 ms or -25...75 ms):
```{r}
df %>%
  filter(roi == 'occipital', 
         pritar == 'primer') %>%
  group_by(group, sbj, shape, time) %>%
  summarise(mean = mean(value)) %>%
  filter(between(time, -50, +50)) %>%
  group_by(group, sbj, shape) %>%
  summarise(mean_amp = mean(mean)) %>%
  spread(shape, mean_amp) %>%
  mutate(shape_diff = nonshape - shape) %>%
  gather(var, value, nonshape:shape_diff) %>%
  ggplot(aes(group, value)) + geom_violin(alpha=.4) + facet_wrap(~var) + theme_bw() +
  ggtitle('-50...50')  +
  scale_y_continuous(limits = c(-10, 10))

df %>%
  filter(roi == 'occipital', 
         pritar == 'primer') %>%
  group_by(group, sbj, shape, time) %>%
  summarise(mean = mean(value)) %>%
  filter(between(time, -25, +75)) %>%
  group_by(group, sbj, shape) %>%
  summarise(mean_amp = mean(mean)) %>%
  spread(shape, mean_amp) %>%
  mutate(shape_diff = nonshape - shape) %>%
  gather(var, value, nonshape:shape_diff) %>%
  ggplot(aes(group, value)) + geom_violin(alpha=.4) + facet_wrap(~var) + theme_bw() +
  ggtitle('-25...75')  +
  scale_y_continuous(limits = c(-10, 10))

```


**TODO**: Select final procedure for mean amplitudes, make models according to RQs.  
